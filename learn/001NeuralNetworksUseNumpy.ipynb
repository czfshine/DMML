{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">这篇文章主要通过几个简单的例子来演示神经网络\n",
    "\n",
    "# 0.目标\n",
    "从一组输入的样本中找到规律，并生成一个程序（函数），接受新的输入，预测结果。\n",
    "# 1.输入输出\n",
    "众所周知，在计算机内部数据是以二进制按顺序存放在存储空间的，**一组有序的数字** 在数学中叫做**向量**，所以对于神经网络而言，可以认为输入都是一个向量，同理，输出也可以当作向量处理。\n",
    "\n",
    ">在编程语言里，通常用一维数组表示向量\n",
    "\n",
    "绝大多数时候，我们所用的数据不止一组，**同时向量组可以用矩阵表示**，所以我们把输入输出都当作矩阵。\n",
    "> 这里涉及一个输入模式输出对的集合（训练数据）。每个输入向量都有一个对应的期望输出向量、或者称作是目标向量。\n",
    "# 2.一个简单的例子\n",
    "考虑输入为$X=\\left(\n",
    "\\begin{array}{ccc}\n",
    " 0 & 0 & 1 \\\\\n",
    " 0 & 1 & 1 \\\\\n",
    " 1 & 0 & 1 \\\\\n",
    " 1 & 1 & 1 \\\\\n",
    "\\end{array}\n",
    "\\right)$，输出为$Y=\\left(\n",
    "\\begin{array}{c}\n",
    " 0 \\\\\n",
    " 1 \\\\\n",
    " 1 \\\\\n",
    " 1 \\\\\n",
    "\\end{array}\n",
    "\\right)$的一组数据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X=np.array([[0,0,1],\n",
    "           [0,1,1],\n",
    "           [1,0,1],\n",
    "           [1,1,1]])\n",
    "Y=np.array([[0,1,1,1]]).transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "我们先动用真正的人工智能（就是我们自己）找下规律：\n",
    "\n",
    "*  显然，第三列对输出没有影响，然后一、二列和输出呈现出跟**或门** 类似的关系。\n",
    "\n",
    "\n",
    "下面我们来让计算机自动的从输入输出中学习这种规律（所谓的机器学习）。\n",
    "现在我们要将一个$4*3$的矩阵映射到$4*1$的矩阵中去，最简单直观的做法的用一个$3*1$的矩阵$W$与$X$相乘。\n",
    "$$Y=XW$$\n",
    "那么，现在的目标就是求这个$W$（称作权值）.\n",
    "\n",
    ">注：在这$W$的大小与输入的向量组的数量（$X$的列数）无关，所以不管有多少组数据，对于这个模型而言$W$还是$3*1$的\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "首先，我们用随机数初始化$W$:\n",
    "$$W=\\left(\n",
    "\\begin{array}{c}\n",
    " \\text{RandomReal} \\\\\n",
    " \\text{RandomReal} \\\\\n",
    " \\text{RandomReal} \\\\\n",
    "\\end{array}\n",
    "\\right)=\\left(\n",
    "\\begin{array}{c}\n",
    " 0.639875 \\\\\n",
    " 0.0906838 \\\\\n",
    " 0.14675 \\\\\n",
    "\\end{array}\n",
    "\\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=np.random.rand(3,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "然后计算$\\hat{Y}=X W=\\left(\n",
    "\\begin{array}{c}\n",
    " 0.14675 \\\\\n",
    " 0.237434 \\\\\n",
    " 0.786625 \\\\\n",
    " 0.877309 \\\\\n",
    "\\end{array}\n",
    "\\right)$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_=X.dot(W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "这里有一个问题，因为$W$元素取值在$R$ 上，所以$\\hat Y $有可能超过$1$，这对于这个模型而言显然是错误的。\n",
    "\n",
    "所以我们要找一个函数$f(\\hat Y)$,使得其输出的范围在$0 \\sim 1$之间。\n",
    "在这里我选用$\\sigma$ 函数，因为$\\sigma^{'}=\\sigma(1-\\sigma) $,它的导数较为简单。\n",
    "\n",
    "$\\sigma $的图像如下：\n",
    "![enter image description here](https://leanote.com/api/file/getImage?fileId=58e051c3ab64413a33003e06)\n",
    "> 选用$\\sigma$的原因不止这一个，后面会介绍具体原因和其他类似的函数\n",
    "\n",
    "\n",
    "\n",
    "所以现在\n",
    "$$\\hat{Y}=\\sigma( XW)=\\left(\n",
    "\\begin{array}{c}\n",
    " 0.536622 \\\\\n",
    " 0.559081 \\\\\n",
    " 0.687106 \\\\\n",
    " 0.706264 \\\\\n",
    "\\end{array}\n",
    "\\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.53978687e-05 2.68941421e-01 5.00000000e-01 7.31058579e-01\n",
      " 9.99954602e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.72581509],\n",
       "       [0.86279314],\n",
       "       [0.82377759],\n",
       "       [0.91738579]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmod(X):\n",
    "    return 1/(1+np.exp(-X))\n",
    "print(sigmod(np.array([-10,-1,0,1,10])))\n",
    "Y_=sigmod(X.dot(W))\n",
    "Y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "然后我们计算一下当前权值$W$的误差：$\\hat e=Y- \\hat Y=\\left(\n",
    "\\begin{array}{c}\n",
    " -0.536622 \\\\\n",
    " 0.440919 \\\\\n",
    " 0.312894 \\\\\n",
    " 0.293736 \\\\\n",
    "\\end{array}\n",
    "\\right)$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.72581509],\n",
       "       [ 0.13720686],\n",
       "       [ 0.17622241],\n",
       "       [ 0.08261421]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_=Y-Y_\n",
    "e_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "可以看到误差还是相当大的，我们根据这些误差的大小正负更新一下$W$：\n",
    "\n",
    "$$\\Delta W=\\hat e * \\sigma^ {' }(\\hat Y)$$\n",
    "\n",
    "$$W=W+X^T \\Delta W$$\n",
    "> 上面两条式子也称作**Delta规则**，具体推导过程在以后的文章里有介绍，这一篇文章只是引入，不深究细节\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old W: [[0.56866112]\n",
      " [0.86519267]\n",
      " [0.97349257]]\n",
      "new W: [[0.62286541]\n",
      " [0.91067876]\n",
      " [0.89682311]]\n"
     ]
    }
   ],
   "source": [
    "def Dsigmod(X):\n",
    "    return sigmod(X)*(1-sigmod(X))\n",
    "d_W=e_*Dsigmod(Y_)\n",
    "print(\"old W:\",W)\n",
    "W=W+X.transpose().dot(d_W)\n",
    "print(\"new W:\",W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们重复上面的步骤，一直更新$W$，使它趋近于我们所要求.(这个模型较为简单,迭代1000次就行)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old W: [[0.62286541]\n",
      " [0.91067876]\n",
      " [0.89682311]]\n",
      "new W: [[ 8.43929978]\n",
      " [ 8.43957152]\n",
      " [-3.87322625]]\n"
     ]
    }
   ],
   "source": [
    "print(\"old W:\",W)\n",
    "\n",
    "for i in range(1000):\n",
    "    Y_=sigmod(X.dot(W))\n",
    "    e_=Y-Y_\n",
    "    d_W=e_*Dsigmod(Y_)\n",
    "    W=W+X.transpose().dot(d_W)\n",
    "print(\"new W:\",W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于新输入的预测，只要拿到计算好的$W$,对于任意一组新的输入$X_i$,可以求得新的输出$\\hat Y_i=\\sigma( X_i W)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9997839]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xi=np.array([[0,1,0]])\n",
    "Yi_=sigmod(Xi.dot(W))\n",
    "Yi_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3.神经网络\n",
    "\n",
    "上面都在讲矩阵的运算，那么跟神经网络有什么关系？\n",
    "\n",
    "先来看一组例子$X=(0,0,1) \\to Y=(0)$,我们用圆点代表元素项，把输入的每一项与输出连接起来，\n",
    "\n",
    "\n",
    "![title](https://leanote.com/api/file/getImage?fileId=58e0567eab64413779003f4b)\n",
    "\n",
    "然后把$W$的每一项按顺序加到每一条边当做权值，注意到在上面的模型里还有一个$\\sigma$函数，将它当做一个虚拟的元素项（节点），最后画出来就是一个赋权图。\n",
    "\n",
    "![title](https://leanote.com/api/file/getImage?fileId=58e05b41ab64413779004036)\n",
    "\n",
    "对$\\sigma$ 这个节点而言，它接收从$X$传进来的三个值，然后加权求和后，通过$\\sigma$函数生成Y的预测值。这时$\\sigma$节点就好像大脑的一个神经元一样，接收别的神经元的电信号，再产生新的电信号给别的神经元。\n",
    "当多个神经元相互连接，就组成一个**神经网络**。\n",
    "> 或者叫做**感知机**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 4.学习能力\n",
    "\n",
    "对于输入为$X=\\left(\n",
    "\\begin{array}{ccc}\n",
    " 0 & 0 & 1 \\\\\n",
    " 0 & 1 & 1 \\\\\n",
    " 1 & 0 & 1 \\\\\n",
    " 1 & 1 & 1 \\\\\n",
    "\\end{array}\n",
    "\\right)$，输出为$Y=\\left(\n",
    "\\begin{array}{c}\n",
    " 0 \\\\\n",
    " 1 \\\\\n",
    " 1 \\\\\n",
    " 1 \\\\\n",
    "\\end{array}\n",
    "\\right)$的一组数据。我们可以用上面的模型学习其中的规律，并预测，假如我们把输出改成$Y=\\left(\n",
    "\\begin{array}{c}\n",
    " 0 \\\\\n",
    " 1 \\\\\n",
    " 1 \\\\\n",
    " 0 \\\\\n",
    "\\end{array}\n",
    "\\right)$，也就是 一二两列的关系改成**异或**，这个模型就不能成功模拟了。\n",
    "> 可以自行用代码验证，或者利用$\\hat{Y}=\\sigma( X W)$这个式子，用反证法证明不存在一个W，使得对上述的输入输出成立（或者近似成立）。\n",
    "> **它的本质是因为简单的感知机不能解决异或问题**\n",
    "\n",
    "下面是验证的效果：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old W: [[0.13264637]\n",
      " [0.47475026]\n",
      " [0.83667792]]\n",
      "new W: [[-4.57966998e-16]\n",
      " [-4.57966998e-16]\n",
      " [ 5.82867088e-16]]\n"
     ]
    }
   ],
   "source": [
    "X=np.array([[0,0,1],\n",
    "           [0,1,1],\n",
    "           [1,0,1],\n",
    "           [1,1,1]])\n",
    "Y=np.array([[0,1,1,0]]).transpose()\n",
    "W=np.random.rand(3,1)\n",
    "\n",
    "print(\"old W:\",W)\n",
    "\n",
    "for i in range(1000):\n",
    "    Y_=sigmod(X.dot(W))\n",
    "    e_=Y-Y_\n",
    "    d_W=e_*Dsigmod(Y_)\n",
    "    W=W+X.transpose().dot(d_W)\n",
    "print(\"new W:\",W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "最终的结果会陷入一个最优解中，但不是我们所需要的。\n",
    "\n",
    "可以这样理解，因为当前模型的学习能力太弱，无法学习出高级的函数。\n",
    "\n",
    "> 如果一个这么简单模型能够学习出所有的函数，那我们还要研究什么。\n",
    "\n",
    "\n",
    "假如我们把模型变得复杂（就是增加神经元），那就能学习出更高级的函数了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5.增加神经元\n",
    "对于这个神经网络：\n",
    "![title](https://leanote.com/api/file/getImage?fileId=58e05b41ab64413779004036)\n",
    "\n",
    "我们把$\\sigma$神经元copy一下，增加复杂度\n",
    "\n",
    "![enter image description here](https://leanote.com/api/file/getImage?fileId=58e0c70bab64413779005685)\n",
    "\n",
    "注意到第三行的$\\sigma$,它把第二行的输出当做输入，同样加权求和，输出Y的预测值。\n",
    "\n",
    "下面写出它数学形式\n",
    "> 有了一个模型的数学形式，你就可以用任意的编程语言编写出来了\n",
    "\n",
    "输入为$X=\\left(\n",
    "\\begin{array}{ccc}\n",
    " 0 & 0 & 1 \\\\\n",
    " 0 & 1 & 1 \\\\\n",
    " 1 & 0 & 1 \\\\\n",
    " 1 & 1 & 1 \\\\\n",
    "\\end{array}\n",
    "\\right)$，输出为$Y=\\left(\n",
    "\\begin{array}{c}\n",
    " 0 \\\\\n",
    " 1 \\\\\n",
    " 1 \\\\\n",
    " 0 \\\\\n",
    "\\end{array}\n",
    "\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([[0,0,1],\n",
    "           [0,1,1],\n",
    "           [1,0,1],\n",
    "           [1,1,1]])\n",
    "Y=np.array([[0,1,1,0]]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "令$\\sigma_i=\\sigma(XW_i )\\ \\  \\ \\ i=1,2,3,4$ \n",
    "与上面的同理，对于任意的$W_i$而言，都是大小为3*1的列向量，那么$W=(W_1，W_2，W_3,W_4)$为$3 * 4$的矩阵，同样用随机数初始化。\n",
    "\n",
    "则$L=(\\sigma_1,\\sigma_2,\\sigma_3,\\sigma_4)$是W和X的乘积，也就是4*4的矩阵。\n",
    "\n",
    "我们得到第一条式子\n",
    "$$L=\\sigma(XW)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=np.random.rand(3,4)\n",
    "L=sigmod(X.dot(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "对于上面模型中的$\\sigma$ 节点，我们也可以类似的写出\n",
    "\n",
    "$$\\hat Y =\\sigma (L W_2 )$$\n",
    "\n",
    "其中，$W_2$是指第二行到第三行的权值，与$W=(W_1，W_2，W_3,W_4)$中的$W_2$意义不同。（找不到别的符号表示了，就重复了= =）。这里的$W_2$ 应该是一个4*1的列向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2=np.random.rand(4,1)\n",
    "Y_=sigmod(L.dot(W2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "然后计算下误差$\\hat e=Y- \\hat Y$，更新权值$W_2$.\n",
    "\n",
    "\n",
    "$$\\Delta W_2=\\hat e * \\sigma^ {' }(\\hat Y)$$\n",
    "\n",
    "$$W_2=W_2+L^T \\Delta W_2$$\n",
    "注意到，$W_2=W_2+L^T \\Delta W_2$ 是以$L$的转置与增量相乘，而不与第一个模型一样的用$X^T$,因为**应该用这个神经元的输入与增量相乘**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_=Y-Y_\n",
    "dW2 = e_* Dsigmod(Y_)\n",
    "W2 = W2 +L.transpose().dot(dW2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">或者可以认为$X$的大小不对，没法相乘？？？\n",
    "\n",
    "算出L的误差（可以直接复用上面算好的$\\Delta W_2$）：$\\hat e_L = \\Delta W_2 W_2^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eL_ = dW2.dot(W2.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更新权值$W$也应该用对应的输入输出算出的增量\n",
    "\n",
    "$$\\Delta W=\\hat e_L * \\sigma^ {' }(L)$$\n",
    "$$W=W+X^T \\Delta W$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW= eL_* Dsigmod(L)\n",
    "W= W+X.transpose().dot(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02033941],\n",
       "       [0.97208095],\n",
       "       [0.97153743],\n",
       "       [0.0243601 ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=np.array([[0,0,1],\n",
    "           [0,1,1],\n",
    "           [1,0,1],\n",
    "           [1,1,1]])\n",
    "Y=np.array([[0,1,1,0]]).transpose()\n",
    "W=np.random.rand(3,4)\n",
    "W2=np.random.rand(4,1)\n",
    "for i in range(10000):\n",
    "    L=sigmod(X.dot(W))\n",
    "    Y_=sigmod(L.dot(W2))\n",
    "    e_=Y-Y_\n",
    "    dW2 = e_* Dsigmod(Y_)\n",
    "    eL_ = dW2.dot(W2.transpose())\n",
    "    dW= eL_* Dsigmod(L)\n",
    "    W2 = W2 +L.transpose().dot(dW2)\n",
    "    W= W+X.transpose().dot(dW)\n",
    "L=sigmod(X.dot(W))\n",
    "Y_=sigmod(L.dot(W2))\n",
    "Y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.整理\n",
    "$ X$ 是我们的模型的输入,$Y$  是模型的输出,\n",
    "\n",
    "在第一个模型里,我们从输入到输出经过:\n",
    "$$ X  \\xrightarrow{\\text{W} } Y$$\n",
    "\n",
    "在第二个模型里,我们从输入到输出经过:\n",
    "$$ X  \\xrightarrow{W_1 } L \\xrightarrow{W _{2}}  Y$$\n",
    "## 6.1 分层\n",
    "\n",
    "不管是从X直接到Y,还是要经过L到Y,可以注意到L和Y的值都与上一个变量(X或L)的值相关.这样的层我们称作全连接层."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Input():\n",
    "    def __init__(self,X):\n",
    "        self.X=X\n",
    "        self.outsize=X.shape[1]\n",
    "    def forward(self):\n",
    "        return self.X\n",
    "    \n",
    "class Dense():\n",
    "    def __init__(self,above,outsize,acfn,Dacfn):\n",
    "        self.outsize=outsize\n",
    "        self.acfn=acfn\n",
    "        self.X=above.X\n",
    "        self.W=np.random.rand(above.outsize,outsize)\n",
    "    def forward(self):\n",
    "\n",
    "        return self.acfn(self.X.dot(self.W))\n",
    "    def forback(self,Y):\n",
    "        Y_=self.forwrad(self.X)\n",
    "        e_=Y-Y_\n",
    "        dW = e_* Dacfn(Y_)\n",
    "        self.W = self.W+X.transpose().dot(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([[0,0,1],\n",
    "           [0,1,1],\n",
    "           [1,0,1],\n",
    "           [1,1,1]])\n",
    "Y=np.array([[0,1,1,1]]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmod(X):\n",
    "    return 1/(1+np.exp(-X))\n",
    "def Dsigmod(X):\n",
    "    return sigmod(X)*(1-sigmod(X))\n",
    "\n",
    "input=Input(X)\n",
    "out=Dense(input,1,sigmod,Dsigmod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.65573155],\n",
       "       [0.69979996],\n",
       "       [0.78618794],\n",
       "       [0.81818729]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    list=[]\n",
    "    def add (layer):\n",
    "        list.append(layer)\n",
    "    def train(count):\n",
    "        for i in range (count):\n",
    "            Y=list[-1].forward()\n",
    "            for j in range(list.count,0,-1):\n",
    "                Y=list[j].forback(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
